import streamlit as st
import json
import re
import requests
from typing import Dict, List, Any, Optional, Tuple

# Enhanced data type mapping configuration
DATABRICKS_TYPE_MAPPING = {
    # Common mappings
    'VARCHAR': 'STRING',
    'VARCHAR(255)': 'STRING',
    'CHAR': 'STRING',
    'TEXT': 'STRING',
    'NVARCHAR': 'STRING',
    'NCHAR': 'STRING',
    'CLOB': 'STRING',
    
    # Numeric types
    'INTEGER': 'INT',
    'INT': 'INT',
    'SMALLINT': 'SMALLINT',
    'TINYINT': 'TINYINT',
    'BIGINT': 'BIGINT',
    'FLOAT': 'DOUBLE',
    'REAL': 'FLOAT',
    'DOUBLE': 'DOUBLE',
    'DOUBLE PRECISION': 'DOUBLE',
    'DECIMAL': 'DECIMAL',
    'NUMERIC': 'DECIMAL',
    'NUMBER': 'DECIMAL',
    'MONEY': 'DECIMAL(19,4)',
    'SMALLMONEY': 'DECIMAL(10,4)',
    
    # Boolean and bit types
    'BOOLEAN': 'BOOLEAN',
    'BOOL': 'BOOLEAN',
    'BIT': 'BOOLEAN',  # Key mapping from feedback
    
    # Date and time types
    'DATE': 'DATE',
    'TIME': 'STRING',  # Databricks doesn't have TIME type
    'DATETIME': 'TIMESTAMP',
    'DATETIME2': 'TIMESTAMP',
    'SMALLDATETIME': 'TIMESTAMP',
    'TIMESTAMP': 'TIMESTAMP',
    'TIMESTAMP_TZ': 'TIMESTAMP',
    'TIMESTAMPTZ': 'TIMESTAMP',
    
    # Binary types
    'BINARY': 'BINARY',
    'VARBINARY': 'BINARY',
    'BLOB': 'BINARY',
    'IMAGE': 'BINARY',
    
    # Other types
    'UUID': 'STRING',
    'UNIQUEIDENTIFIER': 'STRING',
    'XML': 'STRING',
    'JSON': 'STRING',
    'JSONB': 'STRING',
}

# Supported Databricks attribute parameters for identity and other features
SUPPORTED_DATABRICKS_ATTRIBUTES = {
    'identity': 'GENERATED BY DEFAULT AS IDENTITY',
    'generated_always': 'GENERATED ALWAYS AS IDENTITY',
    'auto_increment': 'GENERATED BY DEFAULT AS IDENTITY',
    'serial': 'GENERATED BY DEFAULT AS IDENTITY',
}

def parse_physical_model(json_data: Dict[str, Any]) -> Dict[str, Any]:
    """Parse the physical data model JSON and extract relevant information."""
    try:
        model = json_data.get('model', {})
        entities = model.get('entities', [])
        relationships = model.get('relationships', [])
        
        return {
            'entities': entities,
            'relationships': relationships,
            'model_name': model.get('name', 'Unnamed Model')
        }
    except Exception as e:
        st.error(f"Error parsing JSON: {str(e)}")
        return {'entities': [], 'relationships': [], 'model_name': 'Error Model'}

def sanitize_identifier(name: str, sanitize_method: str = "underscore") -> str:
    """
    Sanitize SQL identifiers (table names, column names) to handle spaces and special characters.
    
    Parameters:
        name: The identifier to sanitize
        sanitize_method: Method to use for sanitization:
            - "underscore": Replace spaces and special chars with underscores (Databricks-compatible)
            - "backtick": Quote with backticks (may cause issues with Databricks)
    
    Returns:
        Sanitized identifier
    """
    if not name:
        return name
    
    # Remove any existing quotes if they exist
    name = name.strip('`"')
    
    if sanitize_method == "underscore":
        # Replace spaces, dots, and special characters with underscore
        # Keep alphanumeric and underscore characters only
        # This is especially important for catalog and schema names in Databricks
        return re.sub(r'[^a-zA-Z0-9_]', '_', name)
    else:  # backtick
        return f"`{name}`"

def map_datatype_to_databricks(data_type: str, handle_incompatible: bool = True) -> str:
    """
    Map the data type from the model to Databricks supported types.
    
    Parameters:
        data_type: The original data type
        handle_incompatible: Whether to map incompatible types to compatible ones
    
    Returns:
        Databricks-compatible data type
    """
    if not handle_incompatible:
        return data_type
    
    # Normalize the data type
    data_type_upper = data_type.upper().strip()
    
    # Handle VARCHAR/CHAR with length specification
    if re.match(r'VARCHAR\(\d+\)', data_type_upper) or re.match(r'CHAR\(\d+\)', data_type_upper):
        return 'STRING'
    
    # Handle DECIMAL/NUMERIC with precision and scale
    decimal_match = re.match(r'(DECIMAL|NUMERIC)\((\d+),(\d+)\)', data_type_upper)
    if decimal_match:
        precision, scale = decimal_match.groups()[1], decimal_match.groups()[2]
        return f'DECIMAL({precision},{scale})'
    
    # Handle DECIMAL/NUMERIC with precision only
    decimal_match = re.match(r'(DECIMAL|NUMERIC)\((\d+)\)', data_type_upper)
    if decimal_match:
        precision = decimal_match.groups()[1]
        return f'DECIMAL({precision},0)'
    
    # Try exact match first
    if data_type_upper in DATABRICKS_TYPE_MAPPING:
        return DATABRICKS_TYPE_MAPPING[data_type_upper]
    
    # If no mapping found, return original type
    return data_type

def get_identity_clause(metadata: Dict[str, Any]) -> str:
    """
    Generate identity clause based on attribute metadata.
    
    Parameters:
        metadata: Attribute metadata dictionary
    
    Returns:
        Identity clause string or empty string
    """
    # Check for various identity indicators in metadata
    for key, clause in SUPPORTED_DATABRICKS_ATTRIBUTES.items():
        if metadata.get(key, False) or metadata.get(key.upper(), False):
            return clause
    
    # Check for identity in description or other fields
    description = metadata.get('description', '').lower()
    if 'identity' in description or 'auto increment' in description or 'serial' in description:
        return SUPPORTED_DATABRICKS_ATTRIBUTES['identity']
    
    return ""

def format_column_definition(col_name: str, data_type: str, not_null: bool = False, 
                           comment: str = "", identity_clause: str = "", 
                           max_name_width: int = 20, max_type_width: int = 15) -> str:
    """
    Format a column definition with proper alignment for readability.
    
    Parameters:
        col_name: Column name
        data_type: Data type
        not_null: Whether column is NOT NULL
        comment: Column comment
        identity_clause: Identity clause (e.g., GENERATED BY DEFAULT AS IDENTITY)
        max_name_width: Maximum width for column name padding
        max_type_width: Maximum width for data type padding
    
    Returns:
        Formatted column definition
    """
    # Pad column name and data type for alignment
    padded_name = col_name.ljust(max_name_width)
    padded_type = data_type.ljust(max_type_width)
    
    column_def = f"  {padded_name} {padded_type}"
    
    # Add identity clause if present
    if identity_clause:
        column_def += f" {identity_clause}"
    
    # Add NOT NULL constraint
    if not_null:
        column_def += " NOT NULL"
    
    # Add comment if available
    if comment:
        # Escape single quotes in comment
        escaped_comment = comment.replace("'", "''")
        column_def += f" COMMENT '{escaped_comment}'"
    
    return column_def

def find_relationship_for_entity(entity_name: str, relationships: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Find all relationships for a given entity."""
    entity_relationships = []
    
    for relationship in relationships:
        source = relationship.get('sourceEntity', {})
        target = relationship.get('targetEntity', {})
        
        source_name = source.get('name', '')
        target_name = target.get('name', '')
        
        if source_name == entity_name or target_name == entity_name:
            entity_relationships.append(relationship)
    
    return entity_relationships

def generate_ddl(model_data: Dict[str, Any], include_options: Dict[str, bool]) -> str:
    """Generate DDL statements for Databricks based on the physical model."""
    entities = model_data.get('entities', [])
    relationships = model_data.get('relationships', [])
    model_name = model_data.get('model_name', 'Unnamed Model')
    
    # Get options
    sanitize_method = include_options.get('sanitize_method', 'underscore')
    handle_incompatible_types = include_options.get('handle_incompatible_types', True)
    catalog_name = include_options.get('catalog_name', '').strip()
    schema_name = include_options.get('schema_name', '').strip()
    custom_pk_names = include_options.get('custom_pk_names', {})
    custom_fk_names = include_options.get('custom_fk_names', {})
    
    ddl = [f"-- DDL for {model_name}"]
    ddl.append("-- Note: In Databricks, primary key and foreign key constraints are declarative and not enforced")
    
    # Add information block about Databricks constraints
    if include_options.get('include_constraint_info', True):
        ddl.append("""
-- Important information about Databricks constraints:
-- 1. Primary and foreign key constraints in Databricks are informational only and not enforced
-- 2. They serve as documentation for relationships between tables
-- 3. Databricks does not automatically validate that these constraints are satisfied
-- 4. You must implement data validation in your ETL processes to maintain data integrity
-- For more information, see: https://docs.databricks.com/en/tables/constraints.html
        """)
    
    # Generate CREATE DATABASE if selected
    if include_options.get('create_database', True):
        # Create a safe database name (remove special chars and spaces)
        safe_db_name = re.sub(r'[^a-zA-Z0-9_]', '_', model_name.lower())
        ddl.append(f"CREATE DATABASE IF NOT EXISTS {safe_db_name};")
        ddl.append(f"USE {safe_db_name};")
    
    # Calculate maximum widths for column alignment
    max_col_name_width = 0
    max_data_type_width = 0
    
    for entity in entities:
        attributes = entity.get('attributes', [])
        for attr in attributes:
            attr_name = attr.get('name', '')
            metadata = attr.get('metadata', {})
            data_type = map_datatype_to_databricks(
                metadata.get('Data type', 'STRING'), 
                handle_incompatible_types
            )
            
            sanitized_attr_name = sanitize_identifier(attr_name, sanitize_method)
            max_col_name_width = max(max_col_name_width, len(sanitized_attr_name))
            max_data_type_width = max(max_data_type_width, len(data_type))
    
    # Ensure minimum widths for readability
    max_col_name_width = max(max_col_name_width, 20)
    max_data_type_width = max(max_data_type_width, 15)
    
    # Generate CREATE TABLE statements with inline primary keys
    for entity in entities:
        entity_name = entity.get('name', '')
        attributes = entity.get('attributes', [])
        
        if not entity_name or not attributes:
            continue
        
        # Sanitize table name to handle spaces
        sanitized_entity_name = sanitize_identifier(entity_name, sanitize_method)
        
        # Build fully qualified table name
        full_table_name = sanitized_entity_name
        if schema_name:
            full_table_name = f"{sanitize_identifier(schema_name, sanitize_method)}.{full_table_name}"
        if catalog_name:
            full_table_name = f"{sanitize_identifier(catalog_name, sanitize_method)}.{full_table_name}"
        
        # Start CREATE TABLE statement
        table_stmt = f"CREATE TABLE IF NOT EXISTS {full_table_name} (\n"
        
        # Add columns with proper formatting
        columns = []
        primary_keys = []
        
        for attr in attributes:
            attr_name = attr.get('name', '')
            metadata = attr.get('metadata', {})
            data_type = map_datatype_to_databricks(
                metadata.get('Data type', 'STRING'), 
                handle_incompatible_types
            )
            not_null = metadata.get('Not null', False)
            description = metadata.get('description', '')
            is_pk = metadata.get('PK', False)
            
            # Sanitize column name to handle spaces
            sanitized_attr_name = sanitize_identifier(attr_name, sanitize_method)
            
            # Get identity clause if applicable
            identity_clause = ""
            if is_pk and data_type in ['INT', 'BIGINT', 'SMALLINT', 'TINYINT']:
                identity_clause = get_identity_clause(metadata)
            
            # Format column definition with alignment
            column_def = format_column_definition(
                sanitized_attr_name, data_type, not_null, 
                description if include_options.get('include_comments', True) else "",
                identity_clause,
                max_col_name_width, max_data_type_width
            )
            
            columns.append(column_def)
            
            # Track primary keys for inline constraint
            if is_pk:
                primary_keys.append(sanitized_attr_name)
        
        # Add table columns
        table_stmt += ",\n".join(columns)
        
        # Add inline primary key constraint if enabled and primary keys exist
        if include_options.get('include_pk', True) and primary_keys:
            # Get custom PK name or generate default
            safe_entity_name = re.sub(r'[^a-zA-Z0-9_]', '_', entity_name.lower())
            pk_name = custom_pk_names.get(entity_name, f"pk_{safe_entity_name}")
            
            pk_cols = ', '.join(primary_keys)
            table_stmt += f",\n  CONSTRAINT {pk_name} PRIMARY KEY ({pk_cols})"
        
        # Close table definition
        table_stmt += "\n)"
        
        # Add clustering if selected
        if include_options.get('add_clustering', False) and primary_keys:
            table_stmt += f"\nCLUSTERED BY ({primary_keys[0]})"
        
        # Add table comment if available
        entity_description = entity.get('metadata', {}).get('Description', '')
        if entity_description and include_options.get('include_comments', True):
            # Escape single quotes in description
            escaped_entity_description = entity_description.replace("'", "''")
            table_stmt += f"\nCOMMENT '{escaped_entity_description}'"
        
        # Add Delta format specification
        if include_options.get('use_delta', True):
            table_stmt += f"\nUSING DELTA"
        
        table_stmt += ";"
        ddl.append(table_stmt)
    
    # Add foreign key constraints using ALTER TABLE
    if include_options.get('include_foreign_keys', True):
        ddl.append("\n-- Adding Foreign Key Constraints (Informational, not enforced)")
        ddl.append("-- Note: Foreign key columns must match primary key columns exactly")
        
        # First, build a map of primary keys for each entity
        entity_primary_keys = {}
        for entity in entities:
            entity_name = entity.get('name', '')
            if not entity_name:
                continue
                
            primary_keys = []
            for attr in entity.get('attributes', []):
                if attr.get('metadata', {}).get('PK', False):
                    sanitized_attr_name = sanitize_identifier(attr.get('name', ''), sanitize_method)
                    if sanitized_attr_name:
                        primary_keys.append(sanitized_attr_name)
            
            if primary_keys:
                entity_primary_keys[entity_name] = primary_keys
        
        # Track added foreign key constraints to avoid duplicates
        added_fk_constraints = set()
        
        for relationship in relationships:
            source = relationship.get('sourceEntity', {})
            target = relationship.get('targetEntity', {})
            
            source_name = source.get('name', '')
            target_name = target.get('name', '')
            source_attrs = source.get('attributeNames', [])
            target_attrs = target.get('attributeNames', [])
            
            if not (source_name and target_name and source_attrs and target_attrs):
                continue
            
            # Get the actual primary keys for the source entity
            source_primary_keys = entity_primary_keys.get(source_name, [])
            
            # Sanitize names to handle spaces
            sanitized_source_name = sanitize_identifier(source_name, sanitize_method)
            sanitized_target_name = sanitize_identifier(target_name, sanitize_method)
            sanitized_source_attrs = [sanitize_identifier(attr, sanitize_method) for attr in source_attrs]
            sanitized_target_attrs = [sanitize_identifier(attr, sanitize_method) for attr in target_attrs]
            
            # Validate foreign key relationship
            # For Databricks, we should only reference actual primary key columns
            if include_options.get('validate_fk_columns', True) and source_primary_keys:
                # Filter source attributes to only include primary key columns
                valid_source_attrs = []
                valid_target_attrs = []
                
                for i, source_attr in enumerate(sanitized_source_attrs):
                    if source_attr in source_primary_keys and i < len(sanitized_target_attrs):
                        valid_source_attrs.append(source_attr)
                        valid_target_attrs.append(sanitized_target_attrs[i])
                
                # If no valid attributes found, try single column FK (most common case)
                if not valid_source_attrs and sanitized_source_attrs and sanitized_target_attrs:
                    # Use the first primary key column if it exists
                    if source_primary_keys:
                        valid_source_attrs = [source_primary_keys[0]]
                        valid_target_attrs = [sanitized_target_attrs[0]]
                
                # Use the validated attributes
                if valid_source_attrs and valid_target_attrs:
                    sanitized_source_attrs = valid_source_attrs
                    sanitized_target_attrs = valid_target_attrs
                else:
                    # Skip this relationship if we can't create a valid FK
                    ddl.append(f"-- SKIPPED: Invalid FK relationship {target_name} -> {source_name}")
                    ddl.append(f"--          Foreign key columns must reference actual primary key columns")
                    ddl.append(f"--          Source PK: {', '.join(source_primary_keys) if source_primary_keys else 'None'}")
                    ddl.append(f"--          Attempted FK: {', '.join(sanitized_source_attrs)}")
                    continue
            
            # Build fully qualified table names
            full_source_name = sanitized_source_name
            full_target_name = sanitized_target_name
            
            if schema_name:
                schema_sanitized = sanitize_identifier(schema_name, sanitize_method)
                full_source_name = f"{schema_sanitized}.{full_source_name}"
                full_target_name = f"{schema_sanitized}.{full_target_name}"
            
            if catalog_name:
                catalog_sanitized = sanitize_identifier(catalog_name, sanitize_method)
                full_source_name = f"{catalog_sanitized}.{full_source_name}"
                full_target_name = f"{catalog_sanitized}.{full_target_name}"
            
            # Create constraint name - check for custom name first
            relationship_key = f"{target_name}_{source_name}"
            if relationship_key in custom_fk_names:
                fk_name = custom_fk_names[relationship_key]
            else:
                # Generate default name with attribute names to ensure uniqueness
                safe_source_name = re.sub(r'[^a-zA-Z0-9_]', '_', source_name.lower())
                safe_target_name = re.sub(r'[^a-zA-Z0-9_]', '_', target_name.lower())
                
                # Add the first target attribute to make the constraint name unique
                if sanitized_target_attrs:
                    safe_target_attr = re.sub(r'[^a-zA-Z0-9_]', '_', sanitized_target_attrs[0].lower())
                    fk_name = f"fk_{safe_target_name}_{safe_target_attr}_{safe_source_name}"
                else:
                    fk_name = f"fk_{safe_target_name}_{safe_source_name}"
            
            # Create a unique identifier for this constraint to avoid duplicates
            constraint_key = f"{sanitized_target_name}_{','.join(sanitized_target_attrs)}_{sanitized_source_name}_{','.join(sanitized_source_attrs)}"
            
            # Skip if this constraint has already been added
            if constraint_key in added_fk_constraints:
                continue
                
            added_fk_constraints.add(constraint_key)
            
            alter_stmt = f"ALTER TABLE {full_target_name} ADD CONSTRAINT {fk_name} " \
                        f"FOREIGN KEY ({', '.join(sanitized_target_attrs)}) " \
                        f"REFERENCES {full_source_name}({', '.join(sanitized_source_attrs)});"
            
            ddl.append(alter_stmt)
    
    # Include foreign key constraints as comments if option is enabled
    elif include_options.get('include_fk_comments', True):
        ddl.append("\n-- Foreign Key Relationships (as comments)")
        
        # Track added foreign key comments to avoid duplicates
        added_fk_comments = set()
        
        for relationship in relationships:
            source = relationship.get('sourceEntity', {})
            target = relationship.get('targetEntity', {})
            
            source_name = source.get('name', '')
            target_name = target.get('name', '')
            source_attrs = source.get('attributeNames', [])
            target_attrs = target.get('attributeNames', [])
            
            if not (source_name and target_name and source_attrs and target_attrs):
                continue
                
            # Sanitize names for display in comments
            sanitized_source_name = sanitize_identifier(source_name, sanitize_method)
            sanitized_target_name = sanitize_identifier(target_name, sanitize_method)
            sanitized_source_attr = sanitize_identifier(source_attrs[0], sanitize_method) if source_attrs else ""
            sanitized_target_attr = sanitize_identifier(target_attrs[0], sanitize_method) if target_attrs else ""
            
            # Create a unique identifier for this comment to avoid duplicates
            comment_key = f"{sanitized_target_name}_{sanitized_target_attr}_{sanitized_source_name}_{sanitized_source_attr}"
            
            # Skip if this comment has already been added
            if comment_key in added_fk_comments:
                continue
                
            added_fk_comments.add(comment_key)
            
            # Enhanced relationship description format as requested in feedback
            fk_comment = f"-- Table {sanitized_source_name} {sanitized_source_attr} (PK) - {sanitized_target_name} {sanitized_target_attr} (FK)"
            ddl.append(fk_comment)
    
    # Add validation examples if enabled
    if include_options.get('include_validation_examples', False):
        ddl.append("\n-- Example data validation queries for maintaining data integrity")
        ddl.append("-- These queries can help identify constraint violations that Databricks does not enforce")
        
        # Generate primary key validation examples
        for entity in entities:
            entity_name = entity.get('name', '')
            attributes = entity.get('attributes', [])
            primary_keys = []
            
            for attr in attributes:
                if attr.get('metadata', {}).get('PK', False):
                    sanitized_attr_name = sanitize_identifier(attr.get('name', ''), sanitize_method)
                    if sanitized_attr_name:
                        primary_keys.append(sanitized_attr_name)
            
            if primary_keys and entity_name:
                sanitized_entity_name = sanitize_identifier(entity_name, sanitize_method)
                
                # Build fully qualified table name for validation
                full_table_name = sanitized_entity_name
                if schema_name:
                    full_table_name = f"{sanitize_identifier(schema_name, sanitize_method)}.{full_table_name}"
                if catalog_name:
                    full_table_name = f"{sanitize_identifier(catalog_name, sanitize_method)}.{full_table_name}"
                
                pk_cols = ', '.join(primary_keys)
                
                ddl.append(f"""
-- Validate primary key uniqueness in {full_table_name}
SELECT {pk_cols}, COUNT(*) as count
FROM {full_table_name}
GROUP BY {pk_cols}
HAVING COUNT(*) > 1;
                """)
        
        # Generate foreign key validation examples
        # Track added validation examples to avoid duplicates
        added_validations = set()
        
        for relationship in relationships:
            source = relationship.get('sourceEntity', {})
            target = relationship.get('targetEntity', {})
            
            source_name = source.get('name', '')
            target_name = target.get('name', '')
            source_attrs = source.get('attributeNames', [])
            target_attrs = target.get('attributeNames', [])
            
            if not (source_name and target_name and source_attrs and target_attrs):
                continue
                
            sanitized_source_name = sanitize_identifier(source_name, sanitize_method)
            sanitized_target_name = sanitize_identifier(target_name, sanitize_method)
            sanitized_source_attrs = [sanitize_identifier(attr, sanitize_method) for attr in source_attrs]
            sanitized_target_attrs = [sanitize_identifier(attr, sanitize_method) for attr in target_attrs]
            
            # Build fully qualified table names for validation
            full_source_name = sanitized_source_name
            full_target_name = sanitized_target_name
            
            if schema_name:
                schema_sanitized = sanitize_identifier(schema_name, sanitize_method)
                full_source_name = f"{schema_sanitized}.{full_source_name}"
                full_target_name = f"{schema_sanitized}.{full_target_name}"
            
            if catalog_name:
                catalog_sanitized = sanitize_identifier(catalog_name, sanitize_method)
                full_source_name = f"{catalog_sanitized}.{full_source_name}"
                full_target_name = f"{catalog_sanitized}.{full_target_name}"
            
            if len(sanitized_source_attrs) == 1 and len(sanitized_target_attrs) == 1:
                # Create a unique identifier for this validation to avoid duplicates
                validation_key = f"{sanitized_target_name}_{sanitized_target_attrs[0]}_{sanitized_source_name}_{sanitized_source_attrs[0]}"
                
                # Skip if this validation has already been added
                if validation_key in added_validations:
                    continue
                    
                added_validations.add(validation_key)
                
                ddl.append(f"""
-- Validate foreign key integrity between {full_target_name} and {full_source_name}
SELECT t.*
FROM {full_target_name} t
LEFT JOIN {full_source_name} s ON t.{sanitized_target_attrs[0]} = s.{sanitized_source_attrs[0]}
WHERE t.{sanitized_target_attrs[0]} IS NOT NULL
  AND s.{sanitized_source_attrs[0]} IS NULL;
                """)
    
    return "\n\n".join(ddl)

def show_databricks_info():
    """Show information about Databricks constraints support."""
    st.info("""
    **Databricks Constraints Support**
    
    In Databricks, primary key and foreign key constraints serve primarily as 
    informational constructs rather than enforced rules. This means:
    
    1. These constraints document relationships between tables
    2. They are NOT enforced by Databricks
    3. It's possible to insert data that violates these constraints
    4. They may be used for query optimization, but do not guarantee data integrity
    
    To maintain data integrity, implement validation in your ETL processes or use 
    the generated validation queries.
    
    **Requires Databricks Runtime 11.2+** for foreign key constraint syntax support.
    """)

def show_databricks_attributes_info():
    """Show information about supported Databricks attributes."""
    with st.expander("ℹ️ Supported Databricks Attribute Parameters"):
        st.markdown("""
        **Identity Column Parameters:**
        - `identity`: Generates `GENERATED BY DEFAULT AS IDENTITY`
        - `generated_always`: Generates `GENERATED ALWAYS AS IDENTITY`  
        - `auto_increment`: Generates `GENERATED BY DEFAULT AS IDENTITY`
        - `serial`: Generates `GENERATED BY DEFAULT AS IDENTITY`
        
        **Requirements:**
        - Identity columns must be of type INT, BIGINT, SMALLINT, or TINYINT
        - Only one identity column per table is allowed
        - Identity columns are automatically NOT NULL
        
        **Note:** These parameters should be included in the attribute metadata 
        from your Ellie.ai model for automatic detection.
        """)

def load_sample_data() -> str:
    """Load sample data for the demo."""
    return '''
{
    "model": {
      "modelId": 173,
      "name": "Logistics Hub",
      "description": "Sample logistics database",
      "entities": [
        {
          "id": "91f0817a-bde6-11ef-858c-0242ac170004",
          "name": "Customer",
          "metadata": {
            "Description": "Customer information"
          },
          "attributes": [
            {
              "id": "91eff7aa-bde6-11ef-858c-0242ac170004",
              "name": "customer_id",
              "order": 0,
              "metadata": {
                "FK": false,
                "PK": true,
                "Unique": false,
                "Data type": "BIGINT",
                "description": "Unique customer identifier"
              }
            },
            {
              "id": "custom1",
              "name": "customer_name",
              "order": 1,
              "metadata": {
                "FK": false,
                "PK": false,
                "Not null": true,
                "Data type": "VARCHAR",
                "description": "Full customer name"
              }
            }
          ]
        },
        {
          "id": "91f0704a-bde6-11ef-858c-0242ac170004",
          "name": "Order",
          "metadata": {
            "Description": "Order information"
          },
          "attributes": [
            {
              "id": "91effd0e-bde6-11ef-858c-0242ac170004",
              "name": "order_id",
              "order": 0,
              "metadata": {
                "FK": false,
                "PK": true,
                "Not null": true,
                "Data type": "BIGINT",
                "description": "Unique order identifier"
              }
            },
            {
              "id": "91effe58-bde6-11ef-858c-0242ac170004",
              "name": "customer_id",
              "order": 1,
              "metadata": {
                "FK": true,
                "PK": false,
                "Not null": true,
                "Data type": "BIGINT",
                "description": "Reference to customer"
              }
            }
          ]
        }
      ],
      "relationships": [
        {
          "sourceEntity": {
            "id": "91f0817a-bde6-11ef-858c-0242ac170004",
            "name": "Customer",
            "startType": "one",
            "attributeNames": [
              "customer_id"
            ]
          },
          "targetEntity": {
            "id": "91f0704a-bde6-11ef-858c-0242ac170004",
            "name": "Order",
            "endType": "many",
            "attributeNames": [
              "customer_id"
            ]
          }
        }
      ]
    }
}
'''

def fetch_model_from_api(model_id: str, api_token: str, environment: str = "templates") -> str:
    """Fetch a physical data model from the Ellie.ai API.
    
    Args:
        model_id: The ID of the model to fetch
        api_token: API token for authentication
        environment: The environment slug (e.g., "templates", "app", etc.)
    """
    try:
        url = f"https://{environment}.ellie.ai/api/v1/models/{model_id}?token={api_token}"
        response = requests.get(url)
        
        if response.status_code == 200:
            return response.text
        else:
            error_msg = f"API request failed with status code {response.status_code}"
            if response.text:
                error_msg += f": {response.text}"
            raise Exception(error_msg)
    except Exception as e:
        raise Exception(f"Failed to fetch model: {str(e)}")

def main():
    st.set_page_config(
        page_title="Databricks DDL Generator",
        page_icon="🔮",
        layout="wide"
    )
    
    st.title("🔮 Databricks DDL Generator")
    st.markdown("Generate DDL statements for Databricks based on a physical data model JSON")
    
    # Create columns for layout
    col1, col2 = st.columns([2, 3])
    
    with col1:
        # Basic DDL Options
        st.subheader("📋 DDL Options")
        
        options = {
            "create_database": st.checkbox("Create database", value=True, help="Generate CREATE DATABASE statement"),
            "include_pk": st.checkbox("Include primary keys", value=True, help="Include primary key constraints inline with CREATE TABLE (informational only in Databricks)"),
            "use_delta": st.checkbox("Use Delta format", value=True, help="Specify Delta format for tables"),
        }
        
        # Data Type Handling
        st.subheader("🔧 Data Type Handling")
        options["handle_incompatible_types"] = st.checkbox(
            "Handle non-compatible data types", 
            value=True, 
            help="Automatically convert non-Databricks types (like BIT, TINYINT) to compatible equivalents"
        )
        
        if options["handle_incompatible_types"]:
            with st.expander("📖 View Data Type Mappings"):
                st.markdown("**Common Type Mappings:**")
                mapping_display = {
                    "BIT → BOOLEAN": "Boolean values",
                    "VARCHAR/CHAR → STRING": "Text data",
                    "INTEGER → INT": "Whole numbers", 
                    "FLOAT → DOUBLE": "Decimal numbers",
                    "DATETIME → TIMESTAMP": "Date and time",
                    "MONEY → DECIMAL(19,4)": "Currency values"
                }
                for mapping, desc in mapping_display.items():
                    st.write(f"• **{mapping}**: {desc}")
        
        # Fully Qualified Names
        st.subheader("🏷️ Fully Qualified Names")
        col_cat, col_sch = st.columns(2)
        
        with col_cat:
            options["catalog_name"] = st.text_input(
                "Catalog Name", 
                value="", 
                help="Optional catalog name (e.g., 'env_catalog'). Dots and special characters will be replaced with underscores.",
                placeholder="env_catalog"
            )
        
        with col_sch:
            options["schema_name"] = st.text_input(
                "Schema Name", 
                value="", 
                help="Optional schema name (e.g., 'sales'). Dots and special characters will be replaced with underscores.",
                placeholder="sales"
            )
        
        if options["catalog_name"] or options["schema_name"]:
            example_name = ""
            sanitized_catalog = ""
            sanitized_schema = ""
            
            if options["catalog_name"]:
                sanitized_catalog = sanitize_identifier(options["catalog_name"], "underscore")
                example_name += sanitized_catalog
                if sanitized_catalog != options["catalog_name"]:
                    st.warning(f"⚠️ Catalog name will be sanitized: `{options['catalog_name']}` → `{sanitized_catalog}`")
            
            if options["schema_name"]:
                sanitized_schema = sanitize_identifier(options["schema_name"], "underscore")
                example_name += f".{sanitized_schema}" if example_name else sanitized_schema
                if sanitized_schema != options["schema_name"]:
                    st.warning(f"⚠️ Schema name will be sanitized: `{options['schema_name']}` → `{sanitized_schema}`")
            
            example_name += ".table_name"
            st.info(f"📝 Tables will be created as: `{example_name}`")
        
        # Constraint Approach
        st.subheader("🔗 Constraint Approach")
        constraint_options = ["alter", "comments"]
        constraint_descriptions = {
            "alter": "Add constraints with ALTER TABLE statements",
            "comments": "Include relationships as comments only"
        }
        
        selected_constraint = st.radio(
            "How to include foreign key relationships:",
            constraint_options,
            index=0,
            format_func=lambda x: f"{x.title()} - {constraint_descriptions[x]}"
        )
        
        options["include_foreign_keys"] = selected_constraint == "alter"
        options["include_fk_comments"] = selected_constraint == "comments"
        
        # Foreign key validation option
        if selected_constraint == "alter":
            options["validate_fk_columns"] = st.checkbox(
                "Validate foreign key columns against primary keys",
                value=True,
                help="Ensures foreign keys only reference actual primary key columns. Recommended for Databricks compatibility."
            )
            
            st.info("💡 **Note**: Constraint names now include attribute names to prevent naming conflicts when multiple foreign keys exist between the same tables.")
        
        # Custom Constraint Names
        with st.expander("🏷️ Custom Constraint Names (Optional)"):
            st.markdown("**Override default constraint names:**")
            
            # Initialize session state for custom names if not exists
            if 'custom_pk_names' not in st.session_state:
                st.session_state.custom_pk_names = {}
            if 'custom_fk_names' not in st.session_state:
                st.session_state.custom_fk_names = {}
            
            # Primary Key Names
            st.markdown("**Primary Key Names:**")
            pk_name_input = st.text_input(
                "Format: table_name=constraint_name", 
                key="pk_names",
                placeholder="Customer=pk_customer_unique",
                help="Enter custom primary key names in format: table_name=constraint_name"
            )
            
            if pk_name_input:
                try:
                    table_name, constraint_name = pk_name_input.split('=', 1)
                    st.session_state.custom_pk_names[table_name.strip()] = constraint_name.strip()
                    st.success(f"✅ PK name set: {table_name.strip()} → {constraint_name.strip()}")
                except ValueError:
                    st.error("❌ Invalid format. Use: table_name=constraint_name")
            
            # Foreign Key Names  
            st.markdown("**Foreign Key Names:**")
            fk_name_input = st.text_input(
                "Format: target_table_source_table=constraint_name",
                key="fk_names", 
                placeholder="Order_Customer=fk_order_customer_special",
                help="Enter custom foreign key names in format: target_table_source_table=constraint_name. Note: Default names now include attribute names for uniqueness."
            )
            
            if fk_name_input:
                try:
                    relationship_key, constraint_name = fk_name_input.split('=', 1)
                    st.session_state.custom_fk_names[relationship_key.strip()] = constraint_name.strip()
                    st.success(f"✅ FK name set: {relationship_key.strip()} → {constraint_name.strip()}")
                except ValueError:
                    st.error("❌ Invalid format. Use: target_table_source_table=constraint_name")
            
            options["custom_pk_names"] = st.session_state.custom_pk_names
            options["custom_fk_names"] = st.session_state.custom_fk_names
        
        # Display info about Databricks constraints and attributes
        show_databricks_info()
        show_databricks_attributes_info()
        
        # Additional Options
        st.subheader("⚙️ Additional Options")
        
        options["include_validation_examples"] = st.checkbox(
            "Include validation queries", 
            value=False, 
            help="Include example SQL queries to validate data integrity"
        )
        
        options["include_constraint_info"] = st.checkbox(
            "Include constraint documentation", 
            value=True, 
            help="Include comment block explaining Databricks constraint behavior"
        )
        
        # Simplified identifier sanitization
        use_underscore_sanitization = st.checkbox(
            "Replace spaces with underscores in table/column names", 
            value=True,
            help="Recommended for Databricks compatibility. If unchecked, names with spaces will be quoted."
        )
        
        options["sanitize_method"] = "underscore" if use_underscore_sanitization else "backtick"
        
        if not use_underscore_sanitization:
            st.warning("⚠️ Databricks Delta tables have limitations with special characters. Using underscores is recommended.")
        
        options["include_comments"] = st.checkbox("Include comments", value=True, help="Add comments for tables and columns")
        options["add_clustering"] = st.checkbox("Add clustering", value=False, help="Add CLUSTERED BY clause on primary key for performance")
        
        # Load sample button
        if st.button("📄 Load Sample Data"):
            st.session_state.json_input = load_sample_data()
    
    # Initialize session state variables if they don't exist
    if 'json_input' not in st.session_state:
        st.session_state.json_input = ""
    
    with col2:
        # Add tabs for different input methods
        tab1, tab2 = st.tabs(["Direct JSON Input", "Fetch from Ellie.ai API"])
        
        with tab1:
            # JSON input area
            json_input = st.text_area(
                "Paste your physical data model JSON here:",
                value=st.session_state.json_input,
                height=300,
                key="json_text_area"
            )
            # Update the session state (this happens automatically via the key)
        
        with tab2:
            # API Fetch form
            st.subheader("Fetch from Ellie.ai API")
            
            col_id, col_token = st.columns(2)
            
            with col_id:
                model_id = st.text_input("Model ID", help="The numeric ID of your Ellie.ai model")
            
            with col_token:
                api_token = st.text_input("API Token", type="password", help="Your Ellie.ai API token")
            
            # Add environment selection
            environment = st.text_input("Environment", value="templates", 
                                       help="The Ellie.ai environment slug (e.g., 'templates', 'app', etc.)")
            
            fetch_button = st.button("Fetch Model")
            if fetch_button:
                if not model_id or not api_token:
                    st.error("Please provide both Model ID and API Token")
                else:
                    try:
                        with st.spinner("Fetching model data..."):
                            fetched_json = fetch_model_from_api(model_id, api_token, environment)
                            # Update the session state
                            st.session_state.json_input = fetched_json
                            st.success("Model fetched successfully! View and edit in the Direct JSON Input tab.")
                    except Exception as e:
                        st.error(f"Failed to fetch model: {str(e)}")
    
    # Get the JSON input from session state
    json_input = st.session_state.json_input
    
    if st.button("Generate DDL", type="primary"):
        if not json_input:
            st.error("Please provide a JSON input or fetch from API")
        else:
            try:
                # Parse JSON input
                json_data = json.loads(json_input)
                
                # Parse the physical model
                model_data = parse_physical_model(json_data)
                
                # Generate DDL
                ddl = generate_ddl(model_data, options)
                
                # Display DDL
                st.subheader("Generated DDL Statements")
                st.code(ddl, language="sql")
                
                # Add download button
                safe_filename = re.sub(r'[^a-zA-Z0-9_]', '_', model_data['model_name'].lower())
                st.download_button(
                    label="Download DDL",
                    data=ddl,
                    file_name=f"{safe_filename}_databricks_ddl.sql",
                    mime="text/plain"
                )
                
            except json.JSONDecodeError:
                st.error("Invalid JSON format. Please check your input.")
            except Exception as e:
                st.error(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main() 